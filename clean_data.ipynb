{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca2ff04",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "in this section, the dataset will be cleaned and restricted to have only the neceaasry values to predict if a game will be a hit or a flop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059cc85",
   "metadata": {},
   "source": [
    "check if there are NAN values in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb2284e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank              0\n",
       "Name              0\n",
       "Platform          0\n",
       "Year            271\n",
       "Genre             0\n",
       "Publisher        58\n",
       "NA_Sales          0\n",
       "EU_Sales          0\n",
       "JP_Sales          0\n",
       "Other_Sales       0\n",
       "Global_Sales      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"vgsales.csv\")\n",
    "df.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54722364",
   "metadata": {},
   "source": [
    "### How to Handle NaN values in this project\n",
    "\n",
    "Publisher NAN values\\\n",
    "Since there are 16958 data entries, 58 NaN values correspond Â± to the 0.4% of the dataset.\n",
    "Given this small value, the best soluton is to remove these columns from teh set.\n",
    "\n",
    "Year NaN values\\\n",
    "The number of entries that is missing is much larger than the number of nan values for the publisher, but still not very large, so, the best solution is to infer those values from the other one in the dataset.\n",
    "The values can be calculated with the mean or the median, which one to choode depends by the dataset, if it has a small standard deviation, with nit much outliers, probalby the mean will be good. But if thre is a high standard deviation of if there are multiple big outliers, median will be better because less affected. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e654",
   "metadata": {},
   "source": [
    "below we are going to remove the values of publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af45284f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank              0\n",
       "Name              0\n",
       "Platform          0\n",
       "Year            249\n",
       "Genre             0\n",
       "Publisher         0\n",
       "NA_Sales          0\n",
       "EU_Sales          0\n",
       "JP_Sales          0\n",
       "Other_Sales       0\n",
       "Global_Sales      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df.dropna(subset=[\"Publisher\"])\n",
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58383381",
   "metadata": {},
   "source": [
    "below we are going to analyse standard deviation and outliers to decide what approach to use for calculating the missing years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e1859b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation = 5.832233209586327\n",
      "Number of outliers: 188\n"
     ]
    }
   ],
   "source": [
    "#removing nan values to calculate standard deviation \n",
    "arr_of_all_years = df_cleaned[\"Year\"]\n",
    "arr_of_all_years = arr_of_all_years[np.isnan(arr_of_all_years) == False]\n",
    "arr_of_all_years.isna().sum()\n",
    "standard_deviation = np.std(arr_of_all_years)\n",
    "print(f\"Standard deviation = {standard_deviation}\")\n",
    "\n",
    "#use z-score to find outliers \n",
    "df_cleaned[\"z_score\"] = (df_cleaned[\"Year\"] - df_cleaned[\"Year\"].mean()) / df_cleaned[\"Year\"].std()\n",
    "print(f\"Number of outliers: {(np.abs(df_cleaned[\"z_score\"]) > 3).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52609d",
   "metadata": {},
   "source": [
    "As we can see from the results, both standard deviation and ouliers are small, so we can calcualte the missing years with the mean of the present Year value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4ea25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank            0\n",
       "Name            0\n",
       "Platform        0\n",
       "Year            0\n",
       "Genre           0\n",
       "Publisher       0\n",
       "NA_Sales        0\n",
       "EU_Sales        0\n",
       "JP_Sales        0\n",
       "Other_Sales     0\n",
       "Global_Sales    0\n",
       "z_score         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.fillna(df_cleaned[\"Year\"].mean(), inplace=True)\n",
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ad0a8",
   "metadata": {},
   "source": [
    "### Removing unnecessary data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
