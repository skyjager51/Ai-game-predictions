{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca2ff04",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "in this section, the dataset will be cleaned and restricted to have only the neceaasry values to predict if a game will be a hit or a flop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059cc85",
   "metadata": {},
   "source": [
    "check if there are NAN values in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"vgsales.csv\")\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54722364",
   "metadata": {},
   "source": [
    "### How to Handle NaN values in this project\n",
    "\n",
    "Publisher NAN values\\\n",
    "Since there are 16958 data entries, 58 NaN values correspond ± to the 0.4% of the dataset.\n",
    "Given this small value, the best soluton is to remove these columns from teh set.\n",
    "\n",
    "Year NaN values\\\n",
    "The number of entries that is missing is much larger than the number of nan values for the publisher, but still not very large, so, the best solution is to infer those values from the other one in the dataset.\n",
    "The values can be calculated with the mean or the median, which one to choode depends by the dataset, if it has a small standard deviation, with nit much outliers, probalby the mean will be good. But if thre is a high standard deviation of if there are multiple big outliers, median will be better because less affected. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e654",
   "metadata": {},
   "source": [
    "below we are going to remove the values of publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=[\"Publisher\"])\n",
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58383381",
   "metadata": {},
   "source": [
    "below we are going to analyse standard deviation and outliers to decide what approach to use for calculating the missing years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1859b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing nan values to calculate standard deviation\n",
    "arr_of_all_years = df_cleaned[\"Year\"]\n",
    "arr_of_all_years = arr_of_all_years[np.isnan(arr_of_all_years) == False]\n",
    "arr_of_all_years.isna().sum()\n",
    "standard_deviation = np.std(arr_of_all_years)\n",
    "print(f\"Standard deviation = {standard_deviation}\")\n",
    "\n",
    "#use z-score to find outliers\n",
    "df_cleaned[\"z_score\"] = (df_cleaned[\"Year\"] - df_cleaned[\"Year\"].mean()) / df_cleaned[\"Year\"].std()\n",
    "print(f\"Number of outliers: {(np.abs(df_cleaned[\"z_score\"]) > 3).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52609d",
   "metadata": {},
   "source": [
    "As we can see from the results, both standard deviation and ouliers are small, so we can calcualte the missing years with the mean of the present Year value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.fillna(df_cleaned[\"Year\"].mean(), inplace=True)\n",
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ad0a8",
   "metadata": {},
   "source": [
    "### Removing unnecessary data \n",
    "\n",
    "Not all the columns are necessary to train the model to recognize if a game will be a hit or a flop, too much values can cause overfitting.\\\n",
    "Our team decided to keep only some of the columns, more precisely: Platform, Year, Genre, Publisher, Global Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2cef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the desired columns\n",
    "columns = ['Platform', 'Year', 'Genre', 'Publisher', 'Global_Sales']\n",
    "df_filtered = df_cleaned[columns].copy()\n",
    "\n",
    "print(df_filtered[\"Platform\"].describe())\n",
    "print(df_filtered[\"Genre\"].describe())\n",
    "print(df_filtered[\"Publisher\"].describe())\n",
    "df_filtered.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27cbe2",
   "metadata": {},
   "source": [
    "### Platform column cleaning \n",
    "\n",
    "In the next step, data encoding, to guarantee that a platform is not considered mathematically greater than another, we are going to use one-hot encoding, that will create new columns.\\\n",
    "To reduce the number of generated columns and reduce overfitting, we are going to group platforms that have a few number of games (that will likely not heavily influence the decisions of the model due to the low number of entries) in a single category called other_platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = (df_filtered[\"Platform\"].value_counts().keys().to_list())\n",
    "values = (df_filtered[\"Platform\"].value_counts().values.tolist())\n",
    "platforms = df_filtered[\"Platform\"].to_list()\n",
    "print(platforms)\n",
    "\n",
    "#group platforms that have both published less than 60 games and are also deprecated\n",
    "x = 0\n",
    "for i in platforms:\n",
    "    index = keys.index(i)\n",
    "\n",
    "    if (values[index] <= 60):\n",
    "        platforms[x] = \"Other_platform\"\n",
    "\n",
    "    x += 1\n",
    "\n",
    "df_filtered[\"Platform\"] = platforms\n",
    "\n",
    "#as we can see from the output, we reduce the number of platform from 31 to 23\n",
    "print(df_filtered[\"Platform\"].describe())\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fa625",
   "metadata": {},
   "source": [
    "### Adding Is_hit column\n",
    "\n",
    "Adding the Is_hit column is necessary for applying supervised learning concepts and enabling the model to identify patterns in the data.\n",
    "\n",
    "We’ll use global_sales and platform as criteria.  A game is considered a hit if it’s in the 90th percentile of its platform sales. This approach avoids biases where only large platforms have hits while smaller platforms, even if their games were actually hits due to their size, are considered flops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_filtered.copy()\n",
    "\n",
    "platform_90th_percentile = df_temp.groupby('Platform')['Global_Sales'].quantile(0.90)\n",
    "\n",
    "df_temp = df_temp.merge(\n",
    "    platform_90th_percentile.rename('Platform_Threshold'),\n",
    "    on='Platform',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# replace inplace fillna with assignment to avoid chained-assignment warning\n",
    "\n",
    "df_filtered[\"Is_hit\"] = (df_temp['Global_Sales'] > df_temp['Platform_Threshold']).astype('Int64')\n",
    "df_filtered[\"Is_hit\"].fillna(0, inplace=True)\n",
    "df_filtered.head(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7ea2c",
   "metadata": {},
   "source": [
    "### Saving the filtered dataframe for visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce077832",
   "metadata": {},
   "source": [
    "To do visual analysis we will save the latest non-encoded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76be15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\"filtered_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16f8e0",
   "metadata": {},
   "source": [
    "### Data encoding \n",
    "\n",
    "To make the data really understable by the model, we need to encode them in numerical values and then apply normalization techniques to standardise the values. \n",
    "\n",
    "For the Genre and the Platform columns, we can use One-Hot encoding, due to the fact that they have a low cardinality (both are below 35), we use One-Hot encoding to guarantee that the model will consider all the unordered data with the same importance.\n",
    "\n",
    "For the publisher, that has more than 500 entries, we use label encoder to avpid getting too much columns and overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc67aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#encode the data\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=[\"Genre\", \"Platform\"], drop_first=True, dtype=int)\n",
    "\n",
    "#map the values from bool to int\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_encoded[\"Publisher\"] = label_encoder.fit_transform(df_encoded[\"Publisher\"])\n",
    "\n",
    "df_encoded.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a448f",
   "metadata": {},
   "source": [
    "### Normalize the Data\n",
    "\n",
    "Once we obtain the numerical value of the strings, we need to apply normalization concepts to convert this data into a ragne that helps in preveting the model to consider more important values with higher numbers.\\\n",
    "Values created with one-hot encoding does not get Normalized since they are used only to say true/false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#select the data to normalize\n",
    "data_to_normalize = [\"Year\", \"Publisher\", \"Global_Sales\"]\n",
    "\n",
    "df_encoded[data_to_normalize] = scaler.fit_transform(df_encoded[data_to_normalize])\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2965e",
   "metadata": {},
   "source": [
    "### Save the new .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"cleaned_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
